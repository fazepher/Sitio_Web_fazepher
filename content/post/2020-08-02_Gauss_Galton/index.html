---
title: "The statistical connection between Gauss and Galton"
author: "Fernando A. Zepeda H."
date: "2020-08-02"
output: html_document
bibliography: biblio_gauss_galton.bib
---



<p><em>This entry is an English version of an article originally published in Spanish at the <a href="http://laberintos.itam.mx/wp/wp-content/uploads/2017/02/N37.pdf">37th issue</a> of ITAM students’ mathematics and actuarial science magazine,</em> Laberintos e Infinitos<em>.</em></p>
<p>Both Karl Friedrich Gauss and Sir Francis Galton made big contributions to the development of Statistics. Gauss discovered the method of least squares, not without having a dispute with Legendre. On the other hand, Galton gave us the Law of Regression towards the Mean.{{% marginnote %}}A more politically correct name than his original <i>regression toward mediocrity</i>. {{% /marginnote%}}</p>
<p>This post’s goal is to present both contributions in a brief manner.</p>
<div id="gauss-and-least-squares" class="section level2">
<h2>Gauss and Least Squares</h2>
<p>One of history’s greatest mathematicians, Gauss was born in Brunswick in 1777 and died in Göttingen in 1855. According to <span class="citation">Finkel (1901)</span>, his favorite field was Number Theory, where he proved on three separate ways that every algebraic equation with integer coeficient has a root of the form
<span class="math display">\[a + bi.\]</span>
Nevertheless, Gauss didn’t limit himself to said Theory. Let us just look at his works, published by the Royal Society of Göttingen: (1) <em>Disquisitiones Arithmeticae</em>, (2) Theory of Numbers, (3) Analysis, (4) Geometry and Method of Least Squares, (5) Mathematical Physiscs, (6) Astronomy and (7) <em>Theoria Motus Corporium Coelestium</em>. One could say he was the last mathematician with universal interests; they also included literature and philology <span class="citation">(Finkel 1901)</span>.</p>
<div class="figure">
<img src="/Gauss_Galton/Gauss.PNG" alt="" />
<p class="caption">Karl Frederich Gauss, figure from <span class="citation">Gauss (n.d.)</span></p>
</div>
<p>In Statistics’ history, Gauss holds a very special place, since he discovered the Method of Least Squares. The problem Gauss was facing was, according to his own words <span class="citation">(Plackett 1972)</span>,</p>
<blockquote>
<p>to determine the most probable values of a number of unknown quantities from a <em>larger</em> number of observations depending on them.
<em>Gauss to Olbers. Brunswick, March 24, 1807</em></p>
</blockquote>
<p>For Gauss, the solution laid in minimizing the sum of the squared differences between observed and computed values. This is the same principle that Legendre published in 1805 and that originated the dispute between both men, that Gauss summarizes as follows <span class="citation">(Plackett 1972)</span>:</p>
<blockquote>
<p>… the principle which I have used since 1794, that the sum of squares must be minimized for the best representation of several quantities which cannot all be represented exactly, is also used in Legendre’s work and is most thoroughly developed.
<em>Gauss to Olbers. Brunswick, July 30, 1806</em></p>
</blockquote>
<div id="method" class="section level3">
<h3>Method</h3>
<p>Let
<span class="math display">\[\{(x_1,y_1),(x_2,y_2),\dots,(x_n,y_n)\}\]</span>
be a set of ordered pairs. The problem is to find the straight line that descrive them the best in the sense of least squares. That is, to determine the line <span class="math inline">\(y=\beta_0+\beta_1x\)</span> that minimizes the following function:</p>
<p><span class="math display">\[ \Delta(\beta_0,\beta_1)=(||\underline{e}||_2)^2.\]</span></p>
<p>Here, <span class="math inline">\(||\cdot||_2\)</span> is the 2-norm and <span class="math inline">\(\underline{e}\)</span> is the fit’s error vector defined as:</p>
<p><span class="math display">\[\underline{e}=(e_1,e_2,\dots,e_n) \quad \text{with}\quad e_i=y_i-(\beta_0+\beta_1x_i) \quad \forall i=1,2,\dots,n.\]</span></p>
<p>Therefore, our objective function is</p>
<p><span class="math display">\[\Delta(\beta_0,\beta_1)=\sum_{i=1}^n (y_i-\beta_0-\beta_1x_i)^2.\]</span></p>
<p>We can easily procede to optimize via differentiation and obtain the following <em>normal ecuations system</em>:</p>
<p><span class="math display">\[ n\hat{\beta_0}+\hat{\beta_1}\sum_{i=1}^{n}x_i=\sum_{i=1}^{n}y_i\\
\hat{\beta_0}\left(\sum_{i=1}^{n}x_i\right)+\hat{\beta_1}\sum_{i=1}^{n}x_i^2=\sum_{i=1}^{n}x_iy_i.\]</span></p>
<p>The 2-norm is chose precisely because of this straightforward nature of the differentiation. If you’re curious, try to see what happens if one chooses the sum of errors (1-norm) as the objective function or any <span class="math inline">\(p\)</span>-norm with <span class="math inline">\(p&gt;2\)</span>.</p>
<p>If we have at least two different <span class="math inline">\(x\)</span> values in our observations, then this system has a solution. When we verify second order conditions we get</p>
<p><span class="math display">\[\hat{\beta_1}=\dfrac{\sum\limits_{i=1}^n(x_i-\bar{x})(y_i-\bar{y})}{\sum\limits_{i=1}^n(x_i-\bar{x})^2}\\ \hat{\beta_0}=\bar{y}-\hat{\beta_1}\bar{x}.\]</span></p>
<p>Thus, our <em>Least Squares line</em> is:</p>
<p><span class="math display">\[\hat{y}=\hat{\beta_0}+\hat{\beta_1}x\\[0.3cm]
\hat{y}=\bar{y}+\dfrac{\sum\limits_{i=1}^n(x_i-\bar{x})(y_i-\bar{y})}{\sum\limits_{i=1}^n(x_i-\bar{x})^2}(x-\bar{x}).\]</span></p>
</div>
</div>
<div id="galton-and-regression-toward-the-mean" class="section level2">
<h2>Galton and Regression toward the Mean</h2>
<p>Contrary to least squares, <span class="citation">Stigler (1997)</span> says, the concept of Regression was the result of the efforts of only one individual: Sir Francis Galton. This English anthropologist, cousin to Charles Darwin, was born in Dudeston in 1822 and died in Haslemere in 1911. He studied medicine at Birmingham and Cambridge Hospitals. From 1860 onwards, he devoted solely to scientific research. He published in 1869 one of his greatest work, <em>Hereditary Genius</em> <span class="citation">(vidas n.d.)</span>.</p>
<p>It was precisely in this book where he started to outline the concept by analyzing some `Genius families` like the Bernoulli in mathematics or the Bach in music. Galton, cited by <span class="citation">Stigler (1997)</span>, proclaimed that</p>
<blockquote>
<p>It is a universal rule that the unknown kinsman in any degree of any specified man, is probably more mediocre than he
<em>Francis Galton, 1866</em></p>
</blockquote>
<p>In his analysis this was clear. Galton noted there was a marked tendency for eminence to diminish when it came to the relatives of Jacob Bernoulli or Johan Sebastian Bach, specially as the relationship was more distant. Since measuring genius was too complicated, Sir Francis decided to focus on characteristics like stature <span class="citation">(Stigler 1997)</span>.</p>
<div class="figure">
<img src="/Gauss_Galton/Galton.jpg" alt="" />
<p class="caption">Sir Francis Galton, figure from <span class="citation">vidas (n.d.)</span></p>
</div>
<p>In 1877, he presented his research about it in a lecture before the Royal Institution. Galton observed the statures of 903 adult children and those of their respective 205 parents <span class="citation">(Galton 1886)</span>. After adjusting for sex, Galton verified what had been true for his other experiments on seeds, that</p>
<blockquote>
<p>… the offspring did <em>not</em> tend to resemble their parent seeds in size, but to be always more mediocre than they- to be smaller than their parents, if the parents were large; to be larger than the parents, if the parents were very small.
<span class="citation">Galton (1886)</span></p>
</blockquote>
<p>Moreover, his experiments showed that what he called the <em>mean filial regression towards mediocrity</em> was, in the case of human height, of 2/3.</p>
<div class="figure">
<img src="/Gauss_Galton/Estaturas.png" alt="" />
<p class="caption">Sir Francis Galton’s result plate, figure from <span class="citation">Galton (1886)</span></p>
</div>
<div id="simple-linear-regression-model" class="section level3">
<h3>Simple Linear Regression Model</h3>
<p>Galton wanted to explain children’s height, given their parents’ height. We could think that this relation may be described by a straigth line. That is, let’s call <span class="math inline">\(Y\)</span> one person’s height and <span class="math inline">\(X\)</span> her parents’ averaged height. We have</p>
<p><span class="math display">\[ Y = \beta_0+\beta_1X.\]</span>
However, we know there are a great (infinite?) number of factors that also come into play. This leads us to think in a <em>conditional probability</em> model <span class="math inline">\(F(Y|X)\)</span> where <span class="math inline">\(Y\)</span> is our <em>interest variable</em> and <span class="math inline">\(X\)</span> a covariate that explains it, so we could call it an <em>explanatory variable</em>. The Linear Regression Model assumes that the following relationship holds, where <span class="math inline">\(x\)</span> is a known quantity and <span class="math inline">\(\beta_0,\beta_1\)</span> and <span class="math inline">\(\sigma^2\)</span> are called parameters:</p>
<p><span class="math display">\[Y|X=x\sim N\left(\beta_0+\beta_1x,\sigma^2\right).\]</span></p>
<p>Now, we usually don’t know the values of said parameters. Thus, we need to use statistics and do <em>inference</em> about them, making use of the information that we gather from a set of ordered pairs which we assume were generated independently by this model.</p>
<p>Here lays the statistical conection between Gauss and his method of Least Squares.</p>
</div>
<div id="gauss-markov-theorem" class="section level3">
<h3>Gauss-Markov Theorem</h3>
<p>Let <span class="math inline">\(\{(x_1,y_1),(x_2,y_2),\dots,(x_n,y_n)\}\)</span> be a set of <span class="math inline">\(n\)</span> ordered pairs that satisfy the following:</p>
<ol style="list-style-type: lower-roman">
<li><span class="math inline">\(y_i=\beta_0+\beta_1x_i+\epsilon_i\)</span>;</li>
<li><span class="math inline">\(E[\epsilon_i]=0\)</span>;</li>
<li><span class="math inline">\(Var(\epsilon_i)=\sigma^2\)</span>;</li>
<li><span class="math inline">\(Cov(\epsilon_i,\epsilon_j)=0 \qquad \forall\; i\neq j\)</span>,</li>
</ol>
<p>where the first three are true for all <span class="math inline">\(i=1,2,\dots,n\)</span>.</p>
<p>Then,</p>
<blockquote>
<p><strong>The Least Squares Estimates are the best linear unbiased estimates in the sense of minimal variance</strong></p>
</blockquote>
<p>This theorem shows that the best linear estimates for <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>, in the sense of minimal variance, are precisely <span class="math inline">\(\hat{\beta_0}\)</span> and <span class="math inline">\(\hat{\beta_1}\)</span>, the least square solutions we discussed before. This reinforces the utility of the 2-norm when using the Least Squares Method.</p>
</div>
</div>
<div id="final-thought" class="section level2">
<h2>Final thought</h2>
<p>It is always interesting to see how the work of great mathematicians interwines and connects in particular ways. What started as an ‘intelectual race’ between Gauss and Legendre, ends up surging because of the interest of an anthropologist in studying the genius, why not, of this very same characters and their families! Today, almost a century and a half later, the linear regression is a very powerful model. Its applications are everywhere due to its flexibility and simplicity, desirable qualities of every mathematical model.</p>
</div>
<div id="references" class="section level2">
<h2>References</h2>
<div id="refs">
<div id="ref-Finkel01">
<p>Finkel, B.F. 1901. “Biography: Karl Frederich Gauss.” <em>The American Mathematical Monthly</em> 8 (2): 25–31.</p>
</div>
<div id="ref-Galton1886">
<p>Galton, Francis. 1886. “Regression Towards Mediocrity in Hereditary Stature.” <em>The Journal of the Anthropological Institute of Great Britain and Ireland</em> 15: 246–63.</p>
</div>
<div id="ref-FigGauss">
<p>Gauss, La casa de. n.d. “Karl Frederich Gauss.” Accessed August 17, 2014. <a href="http://lacasadegauss.files.wordpress.com/2010/10/gauss-carl-friedrich.jpg">http://lacasadegauss.files.wordpress.com/2010/10/gauss-carl-friedrich.jpg</a>.</p>
</div>
<div id="ref-Plackett72">
<p>Plackett, R. L. 1972. “Studies in the History of Probability and Statistics. XXIX: The Discovery of the Method of Least Squares.” <em>Biometrika</em> 59 (2): 239–51.</p>
</div>
<div id="ref-Stigler97">
<p>Stigler, Stephen M. 1997. “Regression Towards the Mean, Historically Considered.” <em>Statistical Methods in Medical Research</em> 6: 103–14.</p>
</div>
<div id="ref-BioGalton">
<p>vidas, Biografías y. n.d. “Sir Francis Galton.” Accessed August 17, 2014. <a href="http://www.biografiasyvidas.com/biografia/g/galton.htm">http://www.biografiasyvidas.com/biografia/g/galton.htm</a>.</p>
</div>
</div>
</div>
